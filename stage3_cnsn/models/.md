Hereâ€™s how to correct the one discrepancy in your implementation (in **SelfNorm**) so that it exactly matches Eq. (6) in Tang et al. (â€œCrossNorm and SelfNorm for Generalization under Distribution Shiftsâ€) and thereby fixes the errors youâ€™re seeing.

---

## Summary

Your **TripletAttention** and **CrossNorm** modules are already faithful to the papers. The only needed change is in **SelfNorm**: when reconstructing the normalized features, you must multiply by the *original* standard deviation before applying the learned scale ğ‘”(Î¼,Ïƒ), i.e.:

\[
y \;=\;\frac{x - \mu}{\sigma}\,\bigl(\sigma\,g(\mu,\sigma)\bigr)\;+\;\mu\,f(\mu,\sigma).
\]

In code, that means replacing  
```python
x_selfnorm = x_norm * std_weights + mean * mean_weights
```  
with  
```python
x_selfnorm = x_norm * (std * std_weights) + mean * mean_weights
```  
â€” so that `std * std_weights` is exactly Ïƒâ€²=ÏƒÂ·g(Î¼,Ïƒ).  

---

## 1. The SelfNorm Bug

### 1.1. What the paper specifies  
Tang et al. define SelfNormâ€™s recalibration as (Eq. 6):

\[
\sigma'_A = g(\mu_A,\sigma_A)\,\sigma_A,\quad
\mu'_A = f(\mu_A,\sigma_A)\,\mu_A,
\]
\[
\text{then}\quad y \;=\;\frac{x - \mu_A}{\sigma_A}\,\sigma'_A \;+\;\mu'_A.
\]   

### 1.2. Your current code  
```python
x_norm = (x - mean) / std
x_selfnorm = x_norm * std_weights + mean * mean_weights
```
Here, `std_weights` is your `g(Î¼,Ïƒ)` and `mean_weights` is `f(Î¼,Ïƒ)`, but you never re-apply the original `std`.  

---

## 2. The One-Line Fix

Change the reconstruction line in **SelfNorm.forward** to include the original Ïƒ:

```diff
- # Current (incorrect) reconstruction
- x_selfnorm = x_norm * std_weights + mean * mean_weights

+ # Corrected reconstruction per Eq. (6)
+ x_selfnorm = x_norm * (std * std_weights) + mean * mean_weights
```

That way,  
- `(x - Î¼)/Ïƒ * (ÏƒÂ·g)` becomes `x_norm * (std * std_weights)`  
- `+ Î¼Â·f` remains `+ mean * mean_weights`  

Now your `SelfNorm` perfectly implements the paperâ€™s formula .  

---

## 3. Double-Check the Rest

1. **TripletAttention**: Already matches â€œRotate to Attendâ€ exactlyâ€”no changes needed.  
2. **CrossNorm**: Exactly swaps mean/std across the batch in trainingâ€”also correct.  
3. **Integration**: Wrapping each `InvertedResidual` to insert TA + CNSN is spot on.  

With this one change, your entire â€œMobileNetV2 + TripletAttention + CNSNâ€ stack will be fully faithful to the published methods.