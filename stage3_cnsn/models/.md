Here’s how to correct the one discrepancy in your implementation (in **SelfNorm**) so that it exactly matches Eq. (6) in Tang et al. (“CrossNorm and SelfNorm for Generalization under Distribution Shifts”) and thereby fixes the errors you’re seeing.

---

## Summary

Your **TripletAttention** and **CrossNorm** modules are already faithful to the papers. The only needed change is in **SelfNorm**: when reconstructing the normalized features, you must multiply by the *original* standard deviation before applying the learned scale 𝑔(μ,σ), i.e.:

\[
y \;=\;\frac{x - \mu}{\sigma}\,\bigl(\sigma\,g(\mu,\sigma)\bigr)\;+\;\mu\,f(\mu,\sigma).
\]

In code, that means replacing  
```python
x_selfnorm = x_norm * std_weights + mean * mean_weights
```  
with  
```python
x_selfnorm = x_norm * (std * std_weights) + mean * mean_weights
```  
— so that `std * std_weights` is exactly σ′=σ·g(μ,σ).  

---

## 1. The SelfNorm Bug

### 1.1. What the paper specifies  
Tang et al. define SelfNorm’s recalibration as (Eq. 6):

\[
\sigma'_A = g(\mu_A,\sigma_A)\,\sigma_A,\quad
\mu'_A = f(\mu_A,\sigma_A)\,\mu_A,
\]
\[
\text{then}\quad y \;=\;\frac{x - \mu_A}{\sigma_A}\,\sigma'_A \;+\;\mu'_A.
\]   

### 1.2. Your current code  
```python
x_norm = (x - mean) / std
x_selfnorm = x_norm * std_weights + mean * mean_weights
```
Here, `std_weights` is your `g(μ,σ)` and `mean_weights` is `f(μ,σ)`, but you never re-apply the original `std`.  

---

## 2. The One-Line Fix

Change the reconstruction line in **SelfNorm.forward** to include the original σ:

```diff
- # Current (incorrect) reconstruction
- x_selfnorm = x_norm * std_weights + mean * mean_weights

+ # Corrected reconstruction per Eq. (6)
+ x_selfnorm = x_norm * (std * std_weights) + mean * mean_weights
```

That way,  
- `(x - μ)/σ * (σ·g)` becomes `x_norm * (std * std_weights)`  
- `+ μ·f` remains `+ mean * mean_weights`  

Now your `SelfNorm` perfectly implements the paper’s formula .  

---

## 3. Double-Check the Rest

1. **TripletAttention**: Already matches “Rotate to Attend” exactly—no changes needed.  
2. **CrossNorm**: Exactly swaps mean/std across the batch in training—also correct.  
3. **Integration**: Wrapping each `InvertedResidual` to insert TA + CNSN is spot on.  

With this one change, your entire “MobileNetV2 + TripletAttention + CNSN” stack will be fully faithful to the published methods.